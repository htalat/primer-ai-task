{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "\n",
    "- Write a Python Jupyter notebook\n",
    "- That loads the TXT version of the top 10 books from project Gutenberg\n",
    "( https://www.gutenberg.org/browse/scores/top )\n",
    "and outputs:\n",
    "    - the 10 most common words in these books\n",
    "    - the 10 most common bigrams from the text\n",
    "    - the 3 most frequent bigrams for each book that are unique to that book  \n",
    "\n",
    "if additional python requirements are necessary to run the notebook, provide a `requirements.txt` file that lists them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hassan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hassan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "data = {}\n",
    "\n",
    "filenames = os.listdir('books')\n",
    "filenames.sort()\n",
    "\n",
    "all_bigrams = {}\n",
    "\n",
    "for filename in filenames:\n",
    "\n",
    "    file = open('books/{}'.format(filename), encoding = 'utf8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    cleaned_up = []\n",
    "    list_for_bigrams = []\n",
    "\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in string.punctuation and word not in stop_words and word.isalpha():\n",
    "            cleaned_up.append(word)\n",
    "\n",
    "    fdist_words = FreqDist(cleaned_up)\n",
    "    bigrams = nltk.bigrams(cleaned_up)\n",
    "    \n",
    "    fdist_bigrams = FreqDist(bigrams)\n",
    "    \n",
    "    most_common_words = fdist_words.most_common(10)\n",
    "    most_common_words = list(map(lambda x: x[0], most_common_words))\n",
    "    \n",
    "    most_common_bigrams = fdist_bigrams.most_common(10)\n",
    "    most_common_bigrams = list(map(lambda x: ' '.join(x[0]), most_common_bigrams))\n",
    "    \n",
    "    for bigram in most_common_bigrams:\n",
    "        if bigram in all_bigrams:\n",
    "            all_bigrams[bigram] += 1\n",
    "        else:\n",
    "            all_bigrams[bigram] = 1\n",
    "    \n",
    "    key = filename[2: -4]\n",
    "    data[key] = {}\n",
    "    data[key] = {\n",
    "        'most_common_words': most_common_words,\n",
    "        'most_common_bigrams': most_common_bigrams\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pride and prejudice: \n",
      "\n",
      "\t 10 most common words: elizabeth, could, would, darcy, said, much, bennet, must, bingley, jane\n",
      "\t 10 most common bigrams: lady catherine, miss bingley, miss bennet, said elizabeth, sir william, young man, de bourgh, miss darcy, project gutenberg, colonel fitzwilliam\n",
      "\t 3 most frequent bigrams: lady catherine, miss bingley, miss bennet\n",
      "\n",
      "pen pictures of british battles: \n",
      "\n",
      "\t 10 most common words: project, battle, enemy, men, german, work, british, one, fire, guns\n",
      "\t 10 most common bigrams: project gutenberg, project electronic, electronic works, united states, battle fleet, gutenberg literary, literary archive, archive foundation, machine guns, electronic work\n",
      "\t 3 most frequent bigrams: electronic works, united states, battle fleet\n",
      "\n",
      "the adventures of sherlock holmes: \n",
      "\n",
      "\t 10 most common words: said, upon, holmes, one, would, man, could, little, see, may\n",
      "\t 10 most common bigrams: said holmes, sherlock holmes, project gutenberg, could see, baker street, lord simon, young lady, let us, upon table, project electronic\n",
      "\t 3 most frequent bigrams: said holmes, sherlock holmes, baker street\n",
      "\n",
      "white nights and other stories: \n",
      "\n",
      "\t 10 most common words: one, would, though, vasya, know, said, could, like, man, ivanovitch\n",
      "\t 10 most common bigrams: semyon ivanovitch, every one, arkady ivanovitch, yulian mastakovitch, project gutenberg, let us, fedosey nikolaitch, long time, young man, said vasya\n",
      "\t 3 most frequent bigrams: semyon ivanovitch, every one, arkady ivanovitch\n",
      "\n",
      "foods that will win the war and how to cook them: \n",
      "\n",
      "\t 10 most common words: cup, teaspoon, fat, water, add, salt, milk, cups, tablespoons, meat\n",
      "\t 10 most common bigrams: teaspoon salt, corn syrup, salt teaspoon, cold water, tablespoons fat, cup milk, teaspoon cayenne, baking powder, dry ingredients, bake minutes\n",
      "\t 3 most frequent bigrams: teaspoon salt, corn syrup, salt teaspoon\n",
      "\n",
      "dictionary of battles: \n",
      "\n",
      "\t 10 most common words: fought, war, french, loss, killed, british, general, lost, wounded, defeated\n",
      "\t 10 most common bigrams: war fought, killed wounded, lost killed, loss killed, heavy loss, wars fought, fought august, years war, totally defeated, fought july\n",
      "\t 3 most frequent bigrams: war fought, killed wounded, lost killed\n",
      "\n",
      "the romance of lust: \n",
      "\n",
      "\t 10 most common words: prick, could, would, dear, time, delicious, cunt, one, first, us\n",
      "\t 10 most common bigrams: miss evelyn, miss frankland, could see, oh dear, dear boy, died away, next day, summer house, round waist, delicious cunt\n",
      "\t 3 most frequent bigrams: miss evelyn, miss frankland, oh dear\n",
      "\n",
      "the red cross girls: \n",
      "\n",
      "\t 10 most common words: barbara, one, nona, mildred, could, girls, must, little, dick, girl\n",
      "\t 10 most common bigrams: red cross, lady dorian, colonel dalton, nona davis, barbara meade, project gutenberg, dick thornton, cross girls, mildred thornton, young man\n",
      "\t 3 most frequent bigrams: red cross, lady dorian, colonel dalton\n",
      "\n",
      "alice adventures in the wonderland: \n",
      "\n",
      "\t 10 most common words: said, alice, little, one, project, like, would, went, know, could\n",
      "\t 10 most common bigrams: said alice, mock turtle, march hare, project gutenberg, said king, thought alice, white rabbit, said hatter, said mock, said caterpillar\n",
      "\t 3 most frequent bigrams: said alice, mock turtle, march hare\n",
      "\n",
      "moby dick: \n",
      "\n",
      "\t 10 most common words: whale, one, like, upon, ahab, man, ship, ye, old, would\n",
      "\t 10 most common bigrams: sperm whale, white whale, moby dick, old man, captain ahab, right whale, ye see, project gutenberg, cried ahab, one hand\n",
      "\t 3 most frequent bigrams: sperm whale, white whale, moby dick\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing the results\n",
    "\n",
    "for book in data:\n",
    "    arr = []\n",
    "    name = ' '.join(book.split('_'))\n",
    "    print('{}: \\n'.format(name))\n",
    "    for bigram in data[book]['most_common_bigrams']:\n",
    "        if all_bigrams[bigram] == 1:\n",
    "            arr.append((bigram))\n",
    "            \n",
    "            if len(arr) == 3:\n",
    "                data[book]['unique_bigrams'] = arr\n",
    "                break\n",
    "                \n",
    "    common_words = ', '.join(data[book]['most_common_words'])\n",
    "    most_common_bigrams = ', '.join(data[book]['most_common_bigrams'])\n",
    "    book_bigrams = ', '.join(data[book]['unique_bigrams'])\n",
    "    \n",
    "    print('\\t 10 most common words: {}'.format(common_words))\n",
    "    print('\\t 10 most common bigrams: {}'.format(most_common_bigrams))\n",
    "    print('\\t 3 most frequent bigrams: {}\\n'.format(book_bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
